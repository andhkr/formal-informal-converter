{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d3ede21",
   "metadata": {},
   "source": [
    "Data collected from:\n",
    "Wiki: https://dumps.wikimedia.org/simplewiki/latest/simplewiki-latest-pages-articles-multistream.xml.bz2 \n",
    "book: https://www.kaggle.com/datasets/muennighoff/bookcorpus\n",
    "blog: https://www.kaggle.com/datasets/rtatman/blog-authorship-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becbbde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alonot/Files/College/2025/NLP/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fafd99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_words(s):\n",
    "    return re.findall(r'\\b\\w+\\b', s)\n",
    "\n",
    "def count_words(s):\n",
    "    return len(re.findall(r'\\b\\w+\\b', s))\n",
    "\n",
    "def read_wiki(path = \"./text\") -> pd.DataFrame:\n",
    "    '''creates a Dataframe from wikipedia articles . the Dataframe contains single column named \"formal\" \n",
    "        path is the direcotry which is created by WikiExtractor(https://github.com/attardi/wikiextractor) tool with --json option set.\n",
    "    '''\n",
    "\n",
    "    dir = Path(path)\n",
    "\n",
    "\n",
    "    data = []\n",
    "    def getData(file):\n",
    "        with open(file) as f:\n",
    "            for l in f.readlines():\n",
    "                df_json = json.loads(l)\n",
    "                data.append({\"formal\":df_json[\"text\"]})\n",
    "\n",
    "    for dirent in dir.iterdir():\n",
    "        if not dirent.is_dir():\n",
    "            continue\n",
    "        print(dirent)\n",
    "        for file in dirent.iterdir():\n",
    "            if not file.is_file():\n",
    "                continue\n",
    "            getData(file)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    # s = df[df[\"title\"] == \"American English\"]\n",
    "    # print(s)\n",
    "    # df = df.rename(columns={\"text\": \"formal\"})\n",
    "    df[\"formal\"]  = df[\"formal\"].map(lambda x : x[:x.find(\"References\")])\n",
    "    df[\"formal\"]  = df[\"formal\"].map(lambda x : x[:x.find(\"\\\"This about can be made longer. You can help Wikipedia by [ adding to it]\\\"\")])\n",
    "    df[\"formal\"]  = df[\"formal\"].map(lambda x : x.replace(\"\\n\",\"\"))\n",
    "            \n",
    "    arr =np.array(np.argsort(df[\"formal\"].map(count_words))[::-1])\n",
    "    df = df.iloc[arr].reset_index(drop=True)\n",
    "    df = df[ df[\"formal\"].map(count_words) <= 128].reset_index(drop=True)\n",
    "    df = df[ df[\"formal\"].map(count_words) >= 100].reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfae3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def read_from_book_corpus(path=\"./book/dataset.arrow\", max_sentences = 1000, wordlimit =128):\n",
    "    '''\n",
    "        max_sentences : dataset may be big. This defines how many sentences we should take at max. (Note actual data sample may be very less, eg : for max_sentences = 10000, we may get 1000 data samples each of less than `wordlimit` words)\n",
    "        path : path to .arrow dataset\n",
    "    '''\n",
    "\n",
    "    dataset = Dataset.from_file(path)\n",
    "    print(\"Read dataset... \\n Total size : \", dataset.num_rows)\n",
    "    total = min(dataset.num_rows, max_sentences)\n",
    "    data = []\n",
    "    for skip_pos in tqdm(range(0, total, 100)):\n",
    "        sentence_lst = []\n",
    "        word_count = 0\n",
    "        for sentence in dataset.skip(skip_pos).take(100)['text']:\n",
    "            curr_word_count = count_words(sentence)\n",
    "            word_count += curr_word_count\n",
    "            if word_count >= wordlimit and len(sentence_lst) != 0:\n",
    "                data.append({\"formal\" : \"\".join(sentence_lst)})\n",
    "                word_count = curr_word_count\n",
    "                sentence_lst.clear()\n",
    "            if curr_word_count <= wordlimit:\n",
    "                sentence_lst.append(sentence)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9323d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "@DeprecationWarning\n",
    "def read_news_coupus(path = \"./news/data/\"):\n",
    "    '''\n",
    "    Not using this because dataset was just scrap of whole page . Contained a lot of in consistencies... \n",
    "    instead using blog data\n",
    "    '''\n",
    "    def get_data(file):\n",
    "        with open(file) as f:\n",
    "            s = f.readlines()\n",
    "            for l in f.readlines():\n",
    "                s.append(l.strip())\n",
    "            s = (\" \".join(s))\n",
    "            bar_pos = s.find(\"|||||\") + 5\n",
    "            s = s[bar_pos : s.find(\"|||||\", bar_pos) ]\n",
    "            print(s)\n",
    "                \n",
    "    dir = Path(path)\n",
    "    i = 0\n",
    "    for file in dir.iterdir():\n",
    "        if not file.is_file():\n",
    "            continue\n",
    "        get_data(file)\n",
    "        print()\n",
    "        if i == 20:\n",
    "            return\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "865a9757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_blog(path = \"./blog.csv\"):\n",
    "    df =  pd.read_csv(path)\n",
    "    df = df[[\"text\"]]\n",
    "    df[\"len\"] = df[\"text\"].map(count_words)\n",
    "    df.sort_values(by='len', ascending= False, inplace= True)\n",
    "    df.reset_index(drop=True, inplace= True)\n",
    "    df.drop(columns=[\"len\"], inplace= True)\n",
    "    df.rename(columns={\"text\" : \"formal\"}, inplace=True)\n",
    "    df = df[df[\"formal\"].map(count_words) >= 100]\n",
    "    df = df[df[\"formal\"].map(count_words) <= 128].reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1baec4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_df = read_blog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9c28b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text/AB\n",
      "text/AC\n",
      "text/AA\n"
     ]
    }
   ],
   "source": [
    "wiki_df = read_wiki()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96605e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read dataset... \n",
      " Total size :  74004228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:16<00:00, 61.28it/s]\n"
     ]
    }
   ],
   "source": [
    "book_df = read_from_book_corpus(max_sentences=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06a48fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20091, 1), (12123, 1), (49166, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# news_df = read_news_coupus()\n",
    "wiki_df.shape, book_df.shape, blog_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a98668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 1) (12123, 1) (20000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>London Stansted Airport () is a large passenge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Henry Rider Haggard (1856–1925) was an English...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Llanymynech is a village in Shropshire, Englan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kevin Scully Geer (November 7, 1952 – January ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lea Katherine Thompson (born May 31, 1961) is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52118</th>\n",
       "      <td>the thrift store ?no , the landfill , countere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52119</th>\n",
       "      <td>dan gunned his motorcycle and bumped into her ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52120</th>\n",
       "      <td>the difference was that david had a sling and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52121</th>\n",
       "      <td>by this time , don had covered the distance to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52122</th>\n",
       "      <td>sharianna picked up her bike and looked at the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52123 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  formal\n",
       "0      London Stansted Airport () is a large passenge...\n",
       "1      Henry Rider Haggard (1856–1925) was an English...\n",
       "2      Llanymynech is a village in Shropshire, Englan...\n",
       "3      Kevin Scully Geer (November 7, 1952 – January ...\n",
       "4      Lea Katherine Thompson (born May 31, 1961) is ...\n",
       "...                                                  ...\n",
       "52118  the thrift store ?no , the landfill , countere...\n",
       "52119  dan gunned his motorcycle and bumped into her ...\n",
       "52120  the difference was that david had a sling and ...\n",
       "52121  by this time , don had covered the distance to...\n",
       "52122  sharianna picked up her bike and looked at the...\n",
       "\n",
       "[52123 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_df = blog_df[:20000]\n",
    "book_df = book_df[:20000]\n",
    "wiki_df = wiki_df[:20000]\n",
    "print(wiki_df.shape, book_df.shape, blog_df.shape)\n",
    "df = pd.concat([wiki_df, blog_df, book_df])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "del wiki_df\n",
    "del blog_df\n",
    "del book_df\n",
    "# randomize the dfs\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c57bea99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Clutter   I started to go through my 'treasures' last night, getting ready for the move and I realized that I have a lot of junk.  Well, some of it is junk, I have boxes of toys - old and new - various video game sytems and a large selection of instruments, recording/mixing devices, and all the usually miscellaneous effects/plugs/switches/ect. all which will need to be packed and labeled.  One thing I am finding is that I come across stuff that I haven't been able to locate for the past year or so and then I realize that as soon as I pack it up that I probably will not be able to locate it for another year.         \n"
     ]
    }
   ],
   "source": [
    "s = df.iloc[0][\"formal\"]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b528c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "informal_data = []\n",
    "\n",
    "df = pd.read_csv(\"./formal_collection.csv\", index_col=\"index\", dtype={\"formal\":str,\"informal\":\"str\"}, keep_default_na=False)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
